{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96d280e-ba7b-4a4b-b8c7-7c2273d590b3",
   "metadata": {},
   "source": [
    "# Clinical Notes Processing Pipeline: Comprehensive Overview\n",
    "\n",
    "This pipeline provides a comprehensive workflow for processing, filtering, summarizing, and interacting with clinical notes. It's designed as a Jupyter notebook with interactive widgets that guide users through each step of the processws.\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "The pipeline consists of five main steps, each implemented as a separate module:\n",
    "\n",
    "1. **Data Loading**: Extract clinical notes from CSV files\n",
    "2. **Note Filtering**: Filter notes by MRN, encounter type, and event description\n",
    "3. **AWS Configuration**: Set up AWS Bedrock for AI processing\n",
    "4. **Note Summarization**: Generate individual or combined summaries of clinical notes\n",
    "5. **Chat Interface**: Ask questions about the clinical notes\n",
    "\n",
    "Each step builds on the previous one, but the updated implementation allows users to access any step directly through a button interface.\n",
    "\n",
    "## Step 1: Data Loading\n",
    "\n",
    "**Key Components:**\n",
    "- `initialize_extraction()`: Sets up the UI for loading CSV data\n",
    "- Input fields for CSV path and output directory\n",
    "- \"Load CSV\" button that reads the data and creates the output directory\n",
    "\n",
    "This step loads clinical notes from a CSV file and prepares them for processing. The CSV is expected to contain columns for MRN, encounter type, medical service, and the actual note content.\n",
    "\n",
    "## Step 2: Note Filtering\n",
    "\n",
    "**Key Components:**\n",
    "- `initialize_filtering_widgets()`: Creates UI for filtering notes\n",
    "- `search_mrn()`: Filters notes by MRN\n",
    "- `filter_encounter_type()`: Further filters by encounter type\n",
    "- `filter_event_desc()`: Final filtering by event description\n",
    "- `save_blob_contents()`: Saves filtered notes as text files\n",
    "\n",
    "This step allows users to progressively filter notes by different criteria. The filtered notes are saved as structured text files organized by MRN in the specified output directory.\n",
    "\n",
    "## Step 3: AWS Configuration\n",
    "\n",
    "**Key Components:**\n",
    "- `setup_aws_ui()`: Creates UI for AWS configuration\n",
    "- `sso_login()`: Handles AWS SSO login\n",
    "- `initialize_aws()`: Sets up AWS Bedrock clients\n",
    "\n",
    "This step configures AWS credentials and initializes the Bedrock clients needed for AI processing. It supports SSO login and verifies access to Bedrock models.\n",
    "\n",
    "## Step 4: Note Summarization\n",
    "\n",
    "**Key Components:**\n",
    "- `initialize_summarization_ui()`: Creates UI for summarization\n",
    "- `summarize_with_bedrock()`: Generates summaries using AWS Bedrock\n",
    "- `summarize_combined_notes_chunked()`: Handles large sets of notes by chunking\n",
    "- `filter_notes()`: Filters notes based on include/exclude keywords\n",
    "\n",
    "This step offers two summarization modes:\n",
    "1. **Individual summaries**: Generates a separate summary for each note\n",
    "2. **Combined summary**: Creates a comprehensive summary of all notes\n",
    "\n",
    "The implementation includes:\n",
    "- Model selection from popular Bedrock models\n",
    "- MRN-specific processing\n",
    "- Note filtering by keywords\n",
    "- Saving individual summaries as separate text files\n",
    "\n",
    "## Step 5: Chat Interface\n",
    "\n",
    "**Key Components:**\n",
    "- `initialize_chat_ui()`: Creates UI for chatting with notes\n",
    "- `chat_with_clinical_notes()`: Processes queries using AWS Bedrock\n",
    "- `filter_notes()`: Filters notes based on include/exclude keywords\n",
    "- `show_example_questions()`: Displays example questions\n",
    "\n",
    "This step allows users to ask questions about the clinical notes and receive AI-generated answers. Features include:\n",
    "- Model selection for Q&A\n",
    "- Note filtering by keywords\n",
    "- MRN-specific processing\n",
    "- Example questions for guidance\n",
    "\n",
    "## Supporting Modules\n",
    "\n",
    "### Prompt Templates\n",
    "The `summarization_prompts.py` file contains prompt templates for different models and tasks:\n",
    "- `INDIVIDUAL_NOTE_PROMPTS`: For summarizing individual notes\n",
    "- `CHUNK_SUMMARIZATION_PROMPTS`: For summarizing chunks of notes\n",
    "- `FINAL_SUMMARIZATION_PROMPTS`: For combining chunk summaries\n",
    "- `COMBINED_NOTE_PROMPTS`: For summarizing all notes together\n",
    "- `QA_PROMPTS`: For answering questions about notes\n",
    "\n",
    "### Utility Functions\n",
    "- `get_clinical_notes()`: Reads text files from a directory\n",
    "- `combine_clinical_notes()`: Combines multiple notes with identifiers\n",
    "- `get_model_family()`: Determines the model family from model ID\n",
    "- `read_text_file()`: Reads content from a text file\n",
    "\n",
    "## Data Flow\n",
    "\n",
    "1. Clinical notes are loaded from a CSV file\n",
    "2. Notes are filtered based on user criteria\n",
    "3. Filtered notes are saved as text files organized by MRN\n",
    "4. AWS Bedrock is configured for AI processing\n",
    "5. Notes are summarized individually or collectively\n",
    "6. Summaries are saved as text files\n",
    "7. Users can ask questions about the notes through the chat interface\n",
    "\n",
    "## Enhancements\n",
    "\n",
    "The pipeline has been enhanced with:\n",
    "1. **Step-by-step navigation**: Users can access any step directly\n",
    "2. **MRN-specific processing**: Notes can be organized and processed by patient\n",
    "3. **Note filtering**: Advanced filtering capabilities for both summarization and chat\n",
    "4. **Model selection**: Users can choose from different AI models\n",
    "5. **Individual summary files**: Each summary is saved as a separate text file\n",
    "6. **Improved error handling**: Better feedback and error messages\n",
    "\n",
    "This pipeline provides a comprehensive solution for healthcare professionals to process, summarize, and interact with clinical notes, making it easier to extract insights from large volumes of clinical documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1e4e870-7fe3-45c3-a492-6398ebb7e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"../src\")\n",
    "import html\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# UI components\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# AWS libraries\n",
    "import boto3\n",
    "import subprocess\n",
    "\n",
    "# Import prompts from external file\n",
    "from summarization_prompts import (\n",
    "    INDIVIDUAL_NOTE_PROMPTS,\n",
    "    CHUNK_SUMMARIZATION_PROMPTS,\n",
    "    FINAL_SUMMARIZATION_PROMPTS,\n",
    "    COMBINED_NOTE_PROMPTS,\n",
    "    QA_PROMPTS\n",
    ")\n",
    "\n",
    "# Global variables\n",
    "df = pd.DataFrame()\n",
    "mrn_filtered_df = pd.DataFrame()\n",
    "encounter_filtered_df = pd.DataFrame()\n",
    "final_filtered_df = pd.DataFrame()\n",
    "notes_dir = ''\n",
    "chat_context = None\n",
    "bedrock_runtime = None\n",
    "bedrock = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197b745-31a6-40f9-9acc-03407289979c",
   "metadata": {},
   "source": [
    "## Data Loading and Filtering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f3545b-0531-4d53-81ef-01a26d421d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_extraction():\n",
    "    \"\"\"Initialize the data extraction process with UI components\"\"\"\n",
    "    global df, notes_dir\n",
    "    \n",
    "    # Default paths - adjust these as needed\n",
    "    data_csv_path = '../data/randome_asd_notes_cleaned3.csv'\n",
    "    notes_dir = '../data/notes/extracted_notes_ind_mrn'\n",
    "    \n",
    "    # Create a file path input widget\n",
    "    csv_path_input = widgets.Text(\n",
    "        value=data_csv_path,\n",
    "        placeholder='Path to CSV file',\n",
    "        description='CSV Path:',\n",
    "        layout={'width': '80%'},\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Create an output directory input widget\n",
    "    output_path_input = widgets.Text(\n",
    "        value='../data/notes/extracted_notes_ind_mrn',\n",
    "        placeholder='Path to output directory for notes',\n",
    "        description='Notes Path:',\n",
    "        layout={'width': '80%'},\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Create a load button\n",
    "    load_button = widgets.Button(\n",
    "        description='Load CSV',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    \n",
    "    # Create an output widget for messages\n",
    "    load_output = widgets.Output()\n",
    "    \n",
    "    # Define the button click handler\n",
    "    def on_load_button_click(b):\n",
    "        global df, notes_dir\n",
    "        with load_output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                # Set paths based on user input\n",
    "                data_csv_path = csv_path_input.value\n",
    "                notes_dir = output_path_input.value\n",
    "                \n",
    "                # Create output directory\n",
    "                os.makedirs(notes_dir, exist_ok=True)\n",
    "                \n",
    "                # Load the CSV file\n",
    "                print(f\"Loading data from {data_csv_path}...\")\n",
    "                df = pd.read_csv(data_csv_path)\n",
    "                print(f\"Successfully loaded {len(df)} records from CSV.\")\n",
    "                print(f\"Notes will be saved to: {notes_dir}\")\n",
    "                \n",
    "                # Initialize the filtering widgets after successful load\n",
    "                initialize_filtering_widgets()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV: {e}\")\n",
    "    \n",
    "    # Attach the click handler\n",
    "    load_button.on_click(on_load_button_click)\n",
    "    \n",
    "    # Display the widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Step 1: Load Data</h3>\"),\n",
    "        csv_path_input,\n",
    "        output_path_input,\n",
    "        load_button,\n",
    "        load_output\n",
    "    ]))\n",
    "\n",
    "def initialize_filtering_widgets():\n",
    "    \"\"\"Initialize widgets for filtering clinical notes\"\"\"\n",
    "    # Step 1: MRN Search\n",
    "    mrn_search = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter MRN',\n",
    "        description='MRN:',\n",
    "        disabled=False\n",
    "    )\n",
    "    mrn_search_button = widgets.Button(description='Search MRN')\n",
    "    \n",
    "    # Step 2: Encounter Type Filter\n",
    "    encounter_type_dropdown = widgets.Dropdown(\n",
    "        description='Encounter Type:',\n",
    "        disabled=True\n",
    "    )\n",
    "    encounter_type_button = widgets.Button(\n",
    "        description='Filter Encounter Type', \n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Step 3: Event Description Filter\n",
    "    event_desc_dropdown = widgets.Dropdown(\n",
    "        description='Event Description:',\n",
    "        disabled=True\n",
    "    )\n",
    "    event_desc_button = widgets.Button(\n",
    "        description='Filter Event Description', \n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Output widget\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Define the filtering functions\n",
    "    def search_mrn(b):\n",
    "        global mrn_filtered_df\n",
    "        with output:\n",
    "            clear_output()\n",
    "            mrn = mrn_search.value\n",
    "            if mrn:\n",
    "                mrn_filtered_df = df[df['MRN'].str.contains(mrn, case=False, na=False)]\n",
    "                print(f\"Found {len(mrn_filtered_df)} records for MRN containing '{mrn}'\")\n",
    "                \n",
    "                # Update encounter type dropdown\n",
    "                encounter_type_dropdown.options = ['All'] + sorted(mrn_filtered_df['encounter_type'].unique().tolist())\n",
    "                encounter_type_dropdown.disabled = False\n",
    "                encounter_type_button.disabled = False\n",
    "                \n",
    "                display(mrn_filtered_df)\n",
    "            else:\n",
    "                print(\"Please enter an MRN\")\n",
    "\n",
    "    def filter_encounter_type(b):\n",
    "        global encounter_filtered_df\n",
    "        with output:\n",
    "            clear_output()\n",
    "            encounter_type = encounter_type_dropdown.value\n",
    "            if encounter_type != 'All':\n",
    "                encounter_filtered_df = mrn_filtered_df[mrn_filtered_df['encounter_type'] == encounter_type]\n",
    "            else:\n",
    "                encounter_filtered_df = mrn_filtered_df.copy()\n",
    "            \n",
    "            print(f\"Found {len(encounter_filtered_df)} records for encounter type '{encounter_type}'\")\n",
    "            \n",
    "            # Update event description dropdown\n",
    "            event_desc_dropdown.options = ['All'] + sorted(encounter_filtered_df['event_desc'].unique().tolist())\n",
    "            event_desc_dropdown.disabled = False\n",
    "            event_desc_button.disabled = False\n",
    "            \n",
    "            display(encounter_filtered_df)\n",
    "\n",
    "    def filter_event_desc(b):\n",
    "        global final_filtered_df\n",
    "        with output:\n",
    "            clear_output()\n",
    "            event_desc = event_desc_dropdown.value\n",
    "            if event_desc != 'All':\n",
    "                final_filtered_df = encounter_filtered_df[encounter_filtered_df['event_desc'] == event_desc]\n",
    "            else:\n",
    "                final_filtered_df = encounter_filtered_df.copy()\n",
    "            \n",
    "            print(f\"Found {len(final_filtered_df)} records for event description '{event_desc}'\")\n",
    "            \n",
    "            # Save blob contents as text and HTML files\n",
    "            save_blob_contents(final_filtered_df)\n",
    "            \n",
    "            display(final_filtered_df)\n",
    "    \n",
    "    # Attach button click handlers\n",
    "    mrn_search_button.on_click(search_mrn)\n",
    "    encounter_type_button.on_click(filter_encounter_type)\n",
    "    event_desc_button.on_click(filter_event_desc)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Step 2: Filter Notes</h3>\"),\n",
    "        widgets.HBox([mrn_search, mrn_search_button]),\n",
    "        widgets.HBox([encounter_type_dropdown, encounter_type_button]),\n",
    "        widgets.HBox([event_desc_dropdown, event_desc_button]),\n",
    "        output\n",
    "    ]))\n",
    "\n",
    "def save_blob_contents(filtered_df):\n",
    "    \"\"\"Save blob contents as text files organized by MRN\"\"\"\n",
    "    saved_count = 0\n",
    "    \n",
    "    for _, row in filtered_df.iterrows():\n",
    "        # Create directory for this MRN in the notes folder\n",
    "        mrn_dir = os.path.join(notes_dir, str(row['MRN']))\n",
    "        os.makedirs(mrn_dir, exist_ok=True)\n",
    "        \n",
    "        # Clean up any characters that might be problematic in filenames\n",
    "        encounter_type = str(row['encounter_type']).replace('/', '_').replace('\\\\', '_').replace(' ', '_')\n",
    "        med_service = str(row.get('med_service', 'unknown')).replace('/', '_').replace('\\\\', '_').replace(' ', '_')\n",
    "        \n",
    "        # Create a structured text file with all required fields\n",
    "        structured_filename = f\"{row['MRN']}_{encounter_type}_{med_service}_{row['note_index']}.txt\"\n",
    "        structured_filepath = os.path.join(mrn_dir, structured_filename)\n",
    "        \n",
    "        with open(structured_filepath, 'w') as f:\n",
    "            # Write metadata\n",
    "            f.write(f\"ICD: {row.get('ICD', 'N/A')}\\n\")\n",
    "            f.write(f\"Encounter Type: {row.get('encounter_type', 'N/A')}\\n\")\n",
    "            f.write(f\"Medical Service: {row.get('med_service', 'N/A')}\\n\")\n",
    "            f.write(f\"Reason for Visit: {row.get('reason_for_visit', 'N/A')}\\n\")\n",
    "            f.write(f\"Encounter ID: {row.get('encntr_id', 'N/A')}\\n\")\n",
    "            f.write(f\"Event Description: {row.get('event_desc', 'N/A')}\\n\")\n",
    "            f.write(f\"Event End Datetime: {row.get('event_end_dt_tm', 'N/A')}\\n\")\n",
    "            f.write(f\"MRN: {row['MRN']}\\n\")\n",
    "            f.write(f\"Note Index: {row['note_index']}\\n\\n\")\n",
    "            \n",
    "            # Write the clinical note content\n",
    "            f.write(\"CLINICAL NOTE\\n\")\n",
    "            f.write(\"=============\\n\\n\")\n",
    "            f.write(row['blob_content_clean'])\n",
    "        \n",
    "        saved_count += 1\n",
    "    \n",
    "    print(f\"Saved {saved_count} structured text files in {notes_dir}, organized by MRN\")\n",
    "    \n",
    "    # Return the MRN directory path for the next step (summarization)\n",
    "    return notes_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86367d01-a739-41bf-9fea-3b62097a382c",
   "metadata": {},
   "source": [
    "## AWS Configuration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daaf15e1-948b-493a-b44c-25ad40602a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_aws_ui():\n",
    "    \"\"\"Create UI for AWS setup\"\"\"\n",
    "    # Create input widgets\n",
    "    profile_input = widgets.Text(\n",
    "        value='plm-dev',\n",
    "        placeholder='AWS SSO profile name',\n",
    "        description='AWS Profile:',\n",
    "        layout={'width': '50%'},\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    region_input = widgets.Text(\n",
    "        value='us-west-2',\n",
    "        placeholder='AWS region',\n",
    "        description='AWS Region:',\n",
    "        layout={'width': '50%'},\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    login_button = widgets.Button(\n",
    "        description='Login with SSO',\n",
    "        button_style='primary',\n",
    "        layout={'width': 'auto'}\n",
    "    )\n",
    "    \n",
    "    init_button = widgets.Button(\n",
    "        description='Initialize AWS',\n",
    "        button_style='success',\n",
    "        layout={'width': 'auto'}\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Define button handlers\n",
    "    def on_login_button_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            sso_login(profile_input.value)\n",
    "    \n",
    "    def on_init_button_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            success = initialize_aws(profile_input.value, region_input.value)\n",
    "            if success:\n",
    "                # Initialize the summarization UI\n",
    "                initialize_summarization_ui()\n",
    "    \n",
    "    # Attach handlers\n",
    "    login_button.on_click(on_login_button_click)\n",
    "    init_button.on_click(on_init_button_click)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Step 3: Configure AWS Bedrock</h3>\"),\n",
    "        widgets.HTML(\"<p>Use your AWS SSO credentials to access Bedrock for note summarization</p>\"),\n",
    "        profile_input,\n",
    "        region_input,\n",
    "        widgets.HBox([login_button, init_button]),\n",
    "        output\n",
    "    ]))\n",
    "\n",
    "def sso_login(profile_name=\"default\"):\n",
    "    \"\"\"Attempt to login with AWS SSO\"\"\"\n",
    "    print(f\"Attempting to login with AWS SSO using profile '{profile_name}'...\")\n",
    "    result = subprocess.run(\n",
    "        [\"aws\", \"sso\", \"login\", \"--profile\", profile_name],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… SSO login successful\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ SSO login failed: {result.stderr}\")\n",
    "        print(\"Please run 'aws sso login --profile your-profile' in a terminal\")\n",
    "        return False\n",
    "\n",
    "def initialize_aws(profile_name=\"default\", region_name=\"us-west-2\"):\n",
    "    \"\"\"Initialize AWS clients\"\"\"\n",
    "    global bedrock_runtime, bedrock\n",
    "    \n",
    "    # Create a session with the profile\n",
    "    session = boto3.Session(profile_name=profile_name, region_name=region_name)\n",
    "    \n",
    "    # Create Bedrock clients using the session\n",
    "    try:\n",
    "        bedrock_runtime = session.client('bedrock-runtime')\n",
    "        bedrock = session.client('bedrock')\n",
    "        \n",
    "        # Test the connection\n",
    "        models = bedrock.list_foundation_models()\n",
    "        print(\"Available Bedrock models:\")\n",
    "        \n",
    "        # Print first 10 models\n",
    "        for model in models['modelSummaries'][:10]:\n",
    "            print(f\"- {model['modelId']}\")\n",
    "        print(f\"... and {len(models['modelSummaries']) - 10} more models\")\n",
    "        print(f\"\\nTotal models available: {len(models['modelSummaries'])}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not initialize AWS: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(f\"1. Run 'aws sso login --profile {profile_name}' in a terminal\")\n",
    "        print(f\"2. Check that your profile is configured with the correct region\")\n",
    "        print(f\"3. Verify that your IAM role has permissions to access Bedrock\")\n",
    "        print(f\"4. Make sure Bedrock is available in region '{region_name}'\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c6289-6557-4459-9137-1d4368393540",
   "metadata": {},
   "source": [
    "## File Handling and Note Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "664bff36-3fa4-4008-9d09-13d5d7289ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    \"\"\"Read the content of a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_clinical_notes(directory_path):\n",
    "    \"\"\"Get all text files in a directory.\"\"\"\n",
    "    path = Path(directory_path)\n",
    "    if not path.exists() or not path.is_dir():\n",
    "        raise ValueError(f\"Invalid directory path: {directory_path}\")\n",
    "        \n",
    "    files = {}\n",
    "    for file_path in path.glob(\"**/*.txt\"):  # Search recursively for all .txt files\n",
    "        files[file_path.name] = read_text_file(file_path)\n",
    "        \n",
    "    print(f\"Found {len(files)} text files\")\n",
    "    return files\n",
    "\n",
    "def combine_clinical_notes(clinical_notes):\n",
    "    \"\"\"Combine all clinical notes into a single text with file identifiers.\"\"\"\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    for filename, content in clinical_notes.items():\n",
    "        if content.strip():  # Skip empty files\n",
    "            combined_text += f\"\\n\\n--- CLINICAL NOTE: {filename} ---\\n\\n\"\n",
    "            combined_text += content\n",
    "    \n",
    "    return combined_text.strip()\n",
    "\n",
    "def get_model_family(model_id):\n",
    "    \"\"\"Determine the model family from the model ID.\"\"\"\n",
    "    if model_id.startswith(\"anthropic.claude\"):\n",
    "        # Check if it's a Claude 3.5 model which uses the messages API\n",
    "        if \"claude-3-5\" in model_id:\n",
    "            return \"anthropic.claude-3-5\"\n",
    "        return \"anthropic.claude\"\n",
    "    elif model_id.startswith(\"amazon.titan\"):\n",
    "        return \"amazon.titan\"\n",
    "    elif model_id.startswith(\"meta.llama\"):\n",
    "        return \"meta.llama\"\n",
    "    else:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7dae3-c264-45b0-957c-7092bb44c008",
   "metadata": {},
   "source": [
    "## Chat Interface Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abba7233-c02d-4446-8532-7d8af5475318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chat_ui(directory_path, model_id):\n",
    "    \"\"\"Initialize the chat interface to ask questions about clinical notes,\n",
    "    and populate the query field from predefined healthcare provider questions.\n",
    "    \n",
    "    This updated version defines the missing `chat_with_clinical_notes` function locally,\n",
    "    which uses QA prompt templates to process user queries.\n",
    "    \"\"\"\n",
    "    global chat_context, bedrock_runtime\n",
    "\n",
    "    import os\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    from summarization_prompts import QA_PROMPTS\n",
    "    from healthcare_provider_questions import QUESTION_LIBRARY\n",
    "\n",
    "    # A helper function to determine model family.\n",
    "    def get_model_family(model_id):\n",
    "        if model_id.startswith(\"anthropic.claude\"):\n",
    "            # Check if it's a Claude 3.5 model which uses the messages API\n",
    "            if \"claude-3-5\" in model_id:\n",
    "                return \"anthropic.claude-3-5\"\n",
    "            return \"anthropic.claude\"\n",
    "        elif model_id.startswith(\"amazon.titan\"):\n",
    "            return \"amazon.titan\"\n",
    "        elif model_id.startswith(\"meta.llama\"):\n",
    "            return \"meta.llama\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    # Define the missing chat function\n",
    "    def chat_with_clinical_notes(query, context, model_id=\"anthropic.claude-v2\"):\n",
    "        try:\n",
    "            model_family = get_model_family(model_id)\n",
    "            if model_family not in QA_PROMPTS:\n",
    "                # If using a Claude 3.5 model, fall back to standard Claude prompts\n",
    "                if model_family == \"anthropic.claude-3-5\":\n",
    "                    prompt_template = QA_PROMPTS[\"anthropic.claude\"]\n",
    "                else:\n",
    "                    return f\"Model family {model_family} not supported for Q&A\"\n",
    "            else:\n",
    "                prompt_template = QA_PROMPTS[model_family]\n",
    "                \n",
    "            # Format the prompt with the provided clinical notes context and query\n",
    "            formatted_prompt = prompt_template.format(context=context, query=query)\n",
    "            \n",
    "            # Prepare and invoke the model request based on the model family\n",
    "            if model_family == \"anthropic.claude-3-5\":\n",
    "                # Claude 3.5 models use the messages API format\n",
    "                body = json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 1000,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": formatted_prompt\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "                response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "                response_body = json.loads(response.get(\"body\").read())\n",
    "                return response_body.get(\"content\", [{}])[0].get(\"text\", \"\")\n",
    "                \n",
    "            elif model_id.startswith(\"anthropic.claude\"):\n",
    "                body = json.dumps({\n",
    "                    \"prompt\": f\"\\n\\nHuman: {formatted_prompt}\\n\\nAssistant:\",\n",
    "                    \"max_tokens_to_sample\": 1000,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9\n",
    "                })\n",
    "                response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "                response_body = json.loads(response.get(\"body\").read())\n",
    "                return response_body.get(\"completion\", \"\")\n",
    "                \n",
    "            elif model_id.startswith(\"amazon.titan\"):\n",
    "                body = json.dumps({\n",
    "                    \"inputText\": formatted_prompt,\n",
    "                    \"textGenerationConfig\": {\n",
    "                        \"maxTokenCount\": 1000,\n",
    "                        \"temperature\": 0,\n",
    "                        \"topP\": 0.9\n",
    "                    }\n",
    "                })\n",
    "                response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "                response_body = json.loads(response.get(\"body\").read())\n",
    "                return response_body.get(\"results\", [{}])[0].get(\"outputText\", \"\")\n",
    "                \n",
    "            elif model_id.startswith(\"meta.llama\"):\n",
    "                body = json.dumps({\n",
    "                    \"prompt\": formatted_prompt,\n",
    "                    \"max_gen_len\": 1000,\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0.9\n",
    "                })\n",
    "                response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "                response_body = json.loads(response.get(\"body\").read())\n",
    "                return response_body.get(\"generation\", \"\")\n",
    "                \n",
    "            else:\n",
    "                return f\"Model {model_id} not supported for Q&A\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error processing Q&A: {str(e)}\"\n",
    "\n",
    "    # Extract MRN from directory_path if it exists.\n",
    "    base_path = directory_path\n",
    "    mrn = \"\"\n",
    "    path_parts = directory_path.split(os.sep)\n",
    "    if len(path_parts) > 1:\n",
    "        potential_mrn = path_parts[-1]\n",
    "        if potential_mrn and not potential_mrn.startswith('.'):\n",
    "            mrn = potential_mrn\n",
    "            base_path = os.sep.join(path_parts[:-1])\n",
    "\n",
    "    # Popular Bedrock models.\n",
    "    popular_models = [\n",
    "        \"anthropic.claude-v2\",\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"anthropic.claude-3-5-sonnet-20241022-v2:0\",  # New model\n",
    "        \"anthropic.claude-3-5-haiku-20241022-v1:0\",   # New model\n",
    "        \"meta.llama3-70b-instruct-v1:0\",\n",
    "        \"meta.llama3-3-70b-instruct-v1:0\",            # New model\n",
    "        \"amazon.titan-text-express-v1\"\n",
    "    ]\n",
    "\n",
    "    # Model selector dropdown.\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=popular_models,\n",
    "        value=model_id if model_id in popular_models else popular_models[0],\n",
    "        description=\"Model:\",\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '50%'}\n",
    "    )\n",
    "\n",
    "    # Create provider dropdown from QUESTION_LIBRARY keys.\n",
    "    provider_options = list(QUESTION_LIBRARY.keys())\n",
    "    provider_dropdown = widgets.Dropdown(\n",
    "        options=provider_options,\n",
    "        value=provider_options[0] if provider_options else None,\n",
    "        description=\"Provider:\",\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '50%'}\n",
    "    )\n",
    "\n",
    "    # Initialize question set dropdown based on selected provider.\n",
    "    qs_dict = QUESTION_LIBRARY.get(provider_dropdown.value, {})\n",
    "    qs_options = list(qs_dict.keys()) if qs_dict else []\n",
    "    question_set_dropdown = widgets.Dropdown(\n",
    "        options=qs_options,\n",
    "        value=qs_options[0] if qs_options else None,\n",
    "        description=\"Question Set:\",\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '50%'}\n",
    "    )\n",
    "\n",
    "    # Initialize specific question dropdown based on the question set.\n",
    "    initial_questions = qs_dict.get(question_set_dropdown.value, []) if qs_dict else []\n",
    "    question_dropdown = widgets.Dropdown(\n",
    "        options=initial_questions,\n",
    "        value=initial_questions[0] if initial_questions else \"\",\n",
    "        description=\"Select Question:\",\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '80%'}\n",
    "    )\n",
    "\n",
    "    # Observers to update question set and question options.\n",
    "    def on_provider_change(change):\n",
    "        new_provider = change['new']\n",
    "        qs = QUESTION_LIBRARY.get(new_provider, {})\n",
    "        new_qs_options = list(qs.keys()) if qs else []\n",
    "        question_set_dropdown.options = new_qs_options\n",
    "        if new_qs_options:\n",
    "            first_set = new_qs_options[0]\n",
    "            question_set_dropdown.value = first_set\n",
    "            question_dropdown.options = qs[first_set]\n",
    "            question_dropdown.value = qs[first_set][0] if qs[first_set] else \"\"\n",
    "        else:\n",
    "            question_set_dropdown.options = []\n",
    "            question_dropdown.options = []\n",
    "            question_dropdown.value = \"\"\n",
    "\n",
    "    def on_question_set_change(change):\n",
    "        current_provider = provider_dropdown.value\n",
    "        qs = QUESTION_LIBRARY.get(current_provider, {})\n",
    "        selected_set = change['new']\n",
    "        if selected_set and selected_set in qs:\n",
    "            question_dropdown.options = qs[selected_set]\n",
    "            question_dropdown.value = qs[selected_set][0] if qs[selected_set] else \"\"\n",
    "        else:\n",
    "            question_dropdown.options = []\n",
    "            question_dropdown.value = \"\"\n",
    "\n",
    "    def on_question_select(change):\n",
    "        if change['new']:\n",
    "            query_input.value = change['new']\n",
    "\n",
    "    provider_dropdown.observe(on_provider_change, names=\"value\")\n",
    "    question_set_dropdown.observe(on_question_set_change, names=\"value\")\n",
    "    question_dropdown.observe(on_question_select, names=\"value\")\n",
    "\n",
    "    # Create UI elements for the chat interface.\n",
    "    chat_header = widgets.HTML(\"<h3>Step 5: Chat with Clinical Notes</h3>\")\n",
    "    chat_description = widgets.HTML(\n",
    "        \"<p>Ask questions about the clinical notes. Responses will be based on the content of the notes.</p>\"\n",
    "    )\n",
    "    base_path_input = widgets.Text(\n",
    "        value=base_path,\n",
    "        placeholder='Enter base path to directory with clinical notes',\n",
    "        description='Base Notes Path:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '80%'}\n",
    "    )\n",
    "    mrn_input = widgets.Text(\n",
    "        value=mrn,\n",
    "        placeholder='Enter MRN',\n",
    "        description='MRN:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '50%'}\n",
    "    )\n",
    "    include_filter = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter keywords to include (comma-separated)',\n",
    "        description='Include notes with:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '80%'}\n",
    "    )\n",
    "    exclude_filter = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter keywords to exclude (comma-separated)',\n",
    "        description='Exclude notes with:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '80%'}\n",
    "    )\n",
    "    list_notes_button = widgets.Button(\n",
    "        description='List Available Notes',\n",
    "        button_style='info',\n",
    "        tooltip='List available notes for the given path/MRN'\n",
    "    )\n",
    "    notes_output = widgets.Output()\n",
    "    load_notes_button = widgets.Button(\n",
    "        description='Load Filtered Notes',\n",
    "        button_style='primary',\n",
    "        tooltip='Load filtered clinical notes for chat'\n",
    "    )\n",
    "    loading_output = widgets.Output()\n",
    "    query_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Ask a question about the clinical notes',\n",
    "        description='Query:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '90%'}\n",
    "    )\n",
    "    chat_button = widgets.Button(\n",
    "        description='Ask',\n",
    "        button_style='success',\n",
    "        tooltip='Submit your question'\n",
    "    )\n",
    "    chat_history = widgets.Output()\n",
    "    clear_chat_button = widgets.Button(\n",
    "        description='Clear Chat',\n",
    "        button_style='warning',\n",
    "        tooltip='Clear chat history'\n",
    "    )\n",
    "    example_output = widgets.Output()\n",
    "    with example_output:\n",
    "        show_example_questions()\n",
    "\n",
    "    # Function to list available notes.\n",
    "    def list_available_notes(b):\n",
    "        with notes_output:\n",
    "            clear_output()\n",
    "            base_path = base_path_input.value\n",
    "            mrn_val = mrn_input.value.strip()\n",
    "            actual_path = os.path.join(base_path, mrn_val) if mrn_val else base_path\n",
    "            if not os.path.exists(actual_path):\n",
    "                print(f\"Directory not found: {actual_path}\")\n",
    "                return\n",
    "            files = list(Path(actual_path).glob(\"*.txt\"))\n",
    "            if not files:\n",
    "                print(f\"No text files found in {actual_path}\")\n",
    "                return\n",
    "            print(f\"Found {len(files)} text files in {actual_path}:\")\n",
    "            for i, file_path in enumerate(files, 1):\n",
    "                print(f\"{i}. {file_path.name}\")\n",
    "            print(\"\\nUse the filter fields above to include or exclude notes based on keywords.\")\n",
    "\n",
    "    # Function to filter notes.\n",
    "    def filter_notes(notes_dict, include_keywords, exclude_keywords):\n",
    "        if not (include_keywords or exclude_keywords):\n",
    "            return notes_dict\n",
    "        filtered_notes = {}\n",
    "        for filename, content in notes_dict.items():\n",
    "            if exclude_keywords:\n",
    "                if any(keyword.lower() in filename.lower() or keyword.lower() in content.lower() for keyword in exclude_keywords):\n",
    "                    continue\n",
    "            if include_keywords:\n",
    "                if any(keyword.lower() in filename.lower() or keyword.lower() in content.lower() for keyword in include_keywords):\n",
    "                    filtered_notes[filename] = content\n",
    "            else:\n",
    "                filtered_notes[filename] = content\n",
    "        return filtered_notes\n",
    "\n",
    "    # Function to load notes into the chat context.\n",
    "    def load_notes(b):\n",
    "        global chat_context\n",
    "        with loading_output:\n",
    "            clear_output()\n",
    "            base_path_val = base_path_input.value\n",
    "            mrn_val = mrn_input.value.strip()\n",
    "            include_keywords = [k.strip() for k in include_filter.value.split(',') if k.strip()]\n",
    "            exclude_keywords = [k.strip() for k in exclude_filter.value.split(',') if k.strip()]\n",
    "            actual_path = os.path.join(base_path_val, mrn_val) if mrn_val else base_path_val\n",
    "            print(f\"Loading clinical notes from {actual_path}...\")\n",
    "            try:\n",
    "                all_clinical_notes = get_clinical_notes(actual_path)\n",
    "                if not all_clinical_notes:\n",
    "                    print(f\"No clinical notes found in {actual_path}\")\n",
    "                    return\n",
    "                if include_keywords or exclude_keywords:\n",
    "                    print(f\"Applying filters - Include: {include_keywords}, Exclude: {exclude_keywords}\")\n",
    "                    clinical_notes = filter_notes(all_clinical_notes, include_keywords, exclude_keywords)\n",
    "                    print(f\"Filtered from {len(all_clinical_notes)} to {len(clinical_notes)} notes\")\n",
    "                else:\n",
    "                    clinical_notes = all_clinical_notes\n",
    "                if not clinical_notes:\n",
    "                    print(\"No notes remain after filtering. Please adjust your filter criteria.\")\n",
    "                    return\n",
    "                chat_context = combine_clinical_notes(clinical_notes)\n",
    "                print(f\"Successfully loaded {len(clinical_notes)} notes. You can now ask questions.\")\n",
    "                if clinical_notes:\n",
    "                    print(\"\\nLoaded files:\")\n",
    "                    for i, filename in enumerate(list(clinical_notes.keys())[:5], 1):\n",
    "                        print(f\"{i}. {filename}\")\n",
    "                    if len(clinical_notes) > 5:\n",
    "                        print(f\"... and {len(clinical_notes) - 5} more files\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading notes: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "    # Chat submission handler.\n",
    "    def on_ask_button_click(b):\n",
    "        query = query_input.value\n",
    "        if not query:\n",
    "            return\n",
    "        if 'chat_context' not in globals() or not chat_context:\n",
    "            with chat_history:\n",
    "                print(\"Please load the clinical notes first using the 'Load Filtered Notes' button\")\n",
    "            return\n",
    "        selected_model = model_dropdown.value\n",
    "        with chat_history:\n",
    "            print(f\"\\nðŸ™‹ You: {query}\")\n",
    "            print(\"\\nðŸ¤– Assistant: \", end=\"\")\n",
    "            response = chat_with_clinical_notes(query, chat_context, model_id=selected_model)\n",
    "            print(response)\n",
    "        query_input.value = ''\n",
    "\n",
    "    # Clear chat handler.\n",
    "    def clear_chat(b):\n",
    "        with chat_history:\n",
    "            chat_history.clear_output()\n",
    "            print(\"Chat history cleared.\")\n",
    "\n",
    "    list_notes_button.on_click(list_available_notes)\n",
    "    load_notes_button.on_click(load_notes)\n",
    "    chat_button.on_click(on_ask_button_click)\n",
    "    clear_chat_button.on_click(clear_chat)\n",
    "\n",
    "    # Display the chat interface.\n",
    "    display(widgets.VBox([\n",
    "        chat_header,\n",
    "        chat_description,\n",
    "        widgets.HTML(\"<h4>Document Selection</h4>\"),\n",
    "        base_path_input,\n",
    "        mrn_input,\n",
    "        list_notes_button,\n",
    "        notes_output,\n",
    "        widgets.HTML(\"<h4>Filter Options</h4>\"),\n",
    "        include_filter,\n",
    "        exclude_filter,\n",
    "        load_notes_button,\n",
    "        loading_output,\n",
    "        widgets.HTML(\"<h4>Predefined Questions</h4>\"),\n",
    "        provider_dropdown,\n",
    "        question_set_dropdown,\n",
    "        question_dropdown,\n",
    "        widgets.HTML(\"<h4>Chat Interface</h4>\"),\n",
    "        widgets.HTML(\"<p>Select a model to use for answering questions, or type your own query below:</p>\"),\n",
    "        model_dropdown,\n",
    "        widgets.HBox([query_input, chat_button]),\n",
    "        chat_history,\n",
    "        clear_chat_button,\n",
    "        example_output\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e71e5e-3670-4f05-aeea-50f4872646b1",
   "metadata": {},
   "source": [
    "## Summarization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "417fae71-d8bf-439b-babd-3a7e8e4d84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_family(model_id):\n",
    "    \"\"\"Determine the model family from the model ID.\"\"\"\n",
    "    if model_id.startswith(\"anthropic.claude\"):\n",
    "        # Check if it's a Claude 3.5 model which uses the messages API\n",
    "        if \"claude-3-5\" in model_id:\n",
    "            return \"anthropic.claude-3-5\"\n",
    "        return \"anthropic.claude\"\n",
    "    elif model_id.startswith(\"amazon.titan\"):\n",
    "        return \"amazon.titan\"\n",
    "    elif model_id.startswith(\"meta.llama\"):\n",
    "        return \"meta.llama\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def summarize_with_bedrock(text, model_id=\"anthropic.claude-v2\", max_tokens=1000):\n",
    "    \"\"\"Generate a summary of the provided text using AWS Bedrock.\"\"\"\n",
    "    try:\n",
    "        # Import prompts from the summarization_prompts module\n",
    "        from summarization_prompts import INDIVIDUAL_NOTE_PROMPTS\n",
    "        \n",
    "        # Determine the model family\n",
    "        model_family = get_model_family(model_id)\n",
    "        \n",
    "        # Get the appropriate prompt template\n",
    "        if model_family in INDIVIDUAL_NOTE_PROMPTS:\n",
    "            prompt_template = INDIVIDUAL_NOTE_PROMPTS[model_family]\n",
    "        else:\n",
    "            # If not found, try the base Claude family\n",
    "            if model_family == \"anthropic.claude-3-5\":\n",
    "                prompt_template = INDIVIDUAL_NOTE_PROMPTS[\"anthropic.claude\"]\n",
    "            else:\n",
    "                return f\"Model family {model_family} not supported for summarization\"\n",
    "        \n",
    "        # Format the prompt with the text\n",
    "        formatted_prompt = prompt_template.format(text=text)\n",
    "        \n",
    "        # Different models have different payload formats\n",
    "        if model_family == \"anthropic.claude-3-5\":\n",
    "            # Claude 3.5 models use the messages API format\n",
    "            body = json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": formatted_prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"content\", [{}])[0].get(\"text\", \"\")\n",
    "            \n",
    "        elif model_id.startswith(\"anthropic.claude\"):\n",
    "            # Standard Claude models (v2, Claude 3 Haiku/Sonnet)\n",
    "            body = json.dumps({\n",
    "                \"prompt\": f\"\\n\\nHuman: {formatted_prompt}\\n\\nAssistant:\",\n",
    "                \"max_tokens_to_sample\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"completion\", \"\")\n",
    "        \n",
    "        elif model_id.startswith(\"amazon.titan\"):\n",
    "            # Amazon Titan models\n",
    "            body = json.dumps({\n",
    "                \"inputText\": formatted_prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": max_tokens,\n",
    "                    \"temperature\": 0,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"results\", [{}])[0].get(\"outputText\", \"\")\n",
    "            \n",
    "        elif model_id.startswith(\"meta.llama\"):\n",
    "            # Meta Llama models\n",
    "            body = json.dumps({\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"generation\", \"\")\n",
    "        \n",
    "        else:\n",
    "            return \"Model not supported for summarization\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "def summarize_chunk(text, model_id, max_tokens):\n",
    "    \"\"\"Summarize a single chunk of notes.\"\"\"\n",
    "    try:\n",
    "        # Import prompts from the summarization_prompts module\n",
    "        from summarization_prompts import CHUNK_SUMMARIZATION_PROMPTS\n",
    "        \n",
    "        # Determine the model family\n",
    "        model_family = get_model_family(model_id)\n",
    "        \n",
    "        # Get the appropriate prompt template\n",
    "        if model_family in CHUNK_SUMMARIZATION_PROMPTS:\n",
    "            prompt_template = CHUNK_SUMMARIZATION_PROMPTS[model_family]\n",
    "        else:\n",
    "            # If not found, try the base Claude family\n",
    "            if model_family == \"anthropic.claude-3-5\":\n",
    "                prompt_template = CHUNK_SUMMARIZATION_PROMPTS[\"anthropic.claude\"]\n",
    "            else:\n",
    "                return f\"Model family {model_family} not supported for summarization\"\n",
    "        \n",
    "        # Format the prompt with the text\n",
    "        formatted_prompt = prompt_template.format(text=text)\n",
    "        \n",
    "        # Handle different model types\n",
    "        if model_family == \"anthropic.claude-3-5\":\n",
    "            # Claude 3.5 models use the messages API format\n",
    "            body = json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": formatted_prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"content\", [{}])[0].get(\"text\", \"\")\n",
    "            \n",
    "        elif model_id.startswith(\"anthropic.claude\"):\n",
    "            body = json.dumps({\n",
    "                \"prompt\": f\"\\n\\nHuman: {formatted_prompt}\\n\\nAssistant:\",\n",
    "                \"max_tokens_to_sample\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"completion\", \"\")\n",
    "        \n",
    "        elif model_id.startswith(\"amazon.titan\"):\n",
    "            body = json.dumps({\n",
    "                \"inputText\": formatted_prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": max_tokens,\n",
    "                    \"temperature\": 0,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"results\", [{}])[0].get(\"outputText\", \"\")\n",
    "            \n",
    "        elif model_id.startswith(\"meta.llama\"):\n",
    "            body = json.dumps({\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"generation\", \"\")\n",
    "        \n",
    "        else:\n",
    "            return f\"Model {model_id} not supported for summarization\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error generating chunk summary: {str(e)}\"\n",
    "\n",
    "def summarize_final(combined_summaries, model_id, max_tokens):\n",
    "    \"\"\"Create a final summary from multiple chunk summaries.\"\"\"\n",
    "    try:\n",
    "        # Import prompts from the summarization_prompts module\n",
    "        from summarization_prompts import FINAL_SUMMARIZATION_PROMPTS\n",
    "        \n",
    "        # Determine the model family\n",
    "        model_family = get_model_family(model_id)\n",
    "        \n",
    "        # Get the appropriate prompt template\n",
    "        if model_family in FINAL_SUMMARIZATION_PROMPTS:\n",
    "            prompt_template = FINAL_SUMMARIZATION_PROMPTS[model_family]\n",
    "        else:\n",
    "            return f\"Model family {model_family} not supported for summarization\"\n",
    "        \n",
    "        # Format the prompt with the text\n",
    "        formatted_prompt = prompt_template.format(text=combined_summaries)\n",
    "        \n",
    "        # Handle different model types\n",
    "        if model_id.startswith(\"anthropic.claude\"):\n",
    "            body = json.dumps({\n",
    "                \"prompt\": f\"\\n\\nHuman: {formatted_prompt}\\n\\nAssistant:\",\n",
    "                \"max_tokens_to_sample\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"completion\", \"\")\n",
    "        \n",
    "        elif model_id.startswith(\"amazon.titan\"):\n",
    "            body = json.dumps({\n",
    "                \"inputText\": formatted_prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": max_tokens,\n",
    "                    \"temperature\": 0,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"results\", [{}])[0].get(\"outputText\", \"\")\n",
    "            \n",
    "        elif model_id.startswith(\"meta.llama\"):\n",
    "            body = json.dumps({\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"generation\", \"\")\n",
    "        \n",
    "        else:\n",
    "            return f\"Model {model_id} not supported for summarization\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error generating final summary: {str(e)}\"\n",
    "\n",
    "def summarize_combined_notes(combined_text, model_id=\"anthropic.claude-v2\", max_tokens=2000):\n",
    "    \"\"\"Generate a comprehensive summary of combined clinical notes.\"\"\"\n",
    "    try:\n",
    "        # Import prompts from the summarization_prompts module\n",
    "        from summarization_prompts import COMBINED_NOTE_PROMPTS\n",
    "        \n",
    "        # Determine the model family\n",
    "        model_family = get_model_family(model_id)\n",
    "        \n",
    "        # Get the appropriate prompt template\n",
    "        if model_family in COMBINED_NOTE_PROMPTS:\n",
    "            prompt_template = COMBINED_NOTE_PROMPTS[model_family]\n",
    "        else:\n",
    "            return f\"Model family {model_family} not supported for summarization\"\n",
    "        \n",
    "        # Format the prompt with the text\n",
    "        formatted_prompt = prompt_template.format(text=combined_text)\n",
    "        \n",
    "        # Handle different model types\n",
    "        if model_id.startswith(\"anthropic.claude\"):\n",
    "            body = json.dumps({\n",
    "                \"prompt\": f\"\\n\\nHuman: {formatted_prompt}\\n\\nAssistant:\",\n",
    "                \"max_tokens_to_sample\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"completion\", \"\")\n",
    "        \n",
    "        elif model_id.startswith(\"amazon.titan\"):\n",
    "            body = json.dumps({\n",
    "                \"inputText\": formatted_prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"maxTokenCount\": max_tokens,\n",
    "                    \"temperature\": 0,\n",
    "                    \"topP\": 0.9\n",
    "                }\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"results\", [{}])[0].get(\"outputText\", \"\")\n",
    "            \n",
    "        elif model_id.startswith(\"meta.llama\"):\n",
    "            body = json.dumps({\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": max_tokens,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.9\n",
    "            })\n",
    "            response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "            return response_body.get(\"generation\", \"\")\n",
    "        \n",
    "        else:\n",
    "            return \"Model not supported for summarization\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "def summarize_combined_notes_chunked(clinical_notes, model_id=\"anthropic.claude-v2\", max_tokens=2000, max_input_tokens=7500):\n",
    "    \"\"\"Generate a comprehensive summary by processing notes in chunks to avoid token limits.\"\"\"\n",
    "    # Sort notes by filename to ensure consistency\n",
    "    sorted_filenames = sorted(clinical_notes.keys())\n",
    "    \n",
    "    # Estimate characters per token (rough approximation)\n",
    "    chars_per_token = 4  # This is an approximation, varies by model\n",
    "    \n",
    "    # Initialize variables\n",
    "    all_summaries = []\n",
    "    current_chunk = \"\"\n",
    "    current_chunk_size = 0\n",
    "    chunk_number = 1\n",
    "    \n",
    "    print(f\"Processing {len(sorted_filenames)} notes in chunks...\")\n",
    "    \n",
    "    # Process notes in chunks\n",
    "    for filename in sorted_filenames:\n",
    "        content = clinical_notes[filename]\n",
    "        if not content.strip():\n",
    "            continue\n",
    "            \n",
    "        note_text = f\"\\n\\n--- CLINICAL NOTE: {filename} ---\\n\\n{content}\"\n",
    "        note_size = len(note_text) // chars_per_token\n",
    "        \n",
    "        # If adding this note would exceed limit, process current chunk\n",
    "        if current_chunk_size + note_size > max_input_tokens and current_chunk:\n",
    "            print(f\"Processing chunk {chunk_number} (approx. {current_chunk_size} tokens)...\")\n",
    "            chunk_summary = summarize_chunk(current_chunk, model_id, max_tokens)\n",
    "            all_summaries.append(chunk_summary)\n",
    "            current_chunk = note_text\n",
    "            current_chunk_size = note_size\n",
    "            chunk_number += 1\n",
    "        else:\n",
    "            # Add to current chunk\n",
    "            current_chunk += note_text\n",
    "            current_chunk_size += note_size\n",
    "    \n",
    "    # Process final chunk if it exists\n",
    "    if current_chunk:\n",
    "        print(f\"Processing final chunk {chunk_number} (approx. {current_chunk_size} tokens)...\")\n",
    "        chunk_summary = summarize_chunk(current_chunk, model_id, max_tokens)\n",
    "        all_summaries.append(chunk_summary)\n",
    "    \n",
    "    # If we have multiple chunk summaries, summarize them together\n",
    "    if len(all_summaries) > 1:\n",
    "        print(\"Generating final summary from all chunk summaries...\")\n",
    "        combined_summaries = \"\\n\\n--- CHUNK SUMMARIES ---\\n\\n\" + \"\\n\\n\".join(all_summaries)\n",
    "        final_summary = summarize_final(combined_summaries, model_id, max_tokens)\n",
    "        return final_summary\n",
    "    elif all_summaries:\n",
    "        return all_summaries[0]\n",
    "    else:\n",
    "        return \"No valid content found to summarize.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8bad5f-ea7b-4651-bbc7-c1b14342251b",
   "metadata": {},
   "source": [
    "## Summarization UI Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3039fe-347b-4ad7-a7a9-ea3a55f349c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_summarization_ui():\n",
    "    \"\"\"Initialize the UI for summarization\"\"\"\n",
    "    # Define commonly used Bedrock models\n",
    "    def initialize_summarization_ui():\n",
    "        \"\"\"Initialize the UI for summarization\"\"\"\n",
    "    # Define commonly used Bedrock models\n",
    "    popular_models = [\n",
    "        \"anthropic.claude-v2\",\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"anthropic.claude-3-5-sonnet-20241022-v2:0\",  # New model\n",
    "        \"anthropic.claude-3-5-haiku-20241022-v1:0\",   # New model\n",
    "        \"meta.llama3-70b-instruct-v1:0\",\n",
    "        \"meta.llama3-3-70b-instruct-v1:0\",            # New model\n",
    "        \"amazon.titan-text-express-v1\"\n",
    "    ]\n",
    "    \n",
    "    # Model selector dropdown\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=popular_models,\n",
    "        value=\"anthropic.claude-v2\",\n",
    "        description=\"Model:\",\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '50%'}\n",
    "    )\n",
    "    \n",
    "    # Base directory path input (defaulting to the notes directory from extraction)\n",
    "    base_path_input = widgets.Text(\n",
    "        value=notes_dir if notes_dir else '../out/notes',\n",
    "        placeholder='Enter base path to directory with clinical notes',\n",
    "        description='Base Notes Path:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '80%'}\n",
    "    )\n",
    "    \n",
    "    # MRN input field\n",
    "    mrn_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter MRN to process specific patient notes',\n",
    "        description='MRN (optional):',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '50%'}\n",
    "    )\n",
    "    \n",
    "    # Summary type selector (individual or combined)\n",
    "    summary_type = widgets.RadioButtons(\n",
    "        options=['Individual summaries', 'Combined summary'],\n",
    "        value='Individual summaries',\n",
    "        description='Summary type:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Max tokens slider\n",
    "    max_tokens = widgets.IntSlider(\n",
    "        value=2000,\n",
    "        min=500,\n",
    "        max=4000,\n",
    "        step=500,\n",
    "        description='Max tokens:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '50%'}\n",
    "    )\n",
    "    \n",
    "    # Filter options\n",
    "    include_filter = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter keywords to include (comma-separated)',\n",
    "        description='Include notes with:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '80%'}\n",
    "    )\n",
    "    \n",
    "    exclude_filter = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter keywords to exclude (comma-separated)',\n",
    "        description='Exclude notes with:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '80%'}\n",
    "    )\n",
    "    \n",
    "    # List notes button\n",
    "    list_notes_button = widgets.Button(\n",
    "        description='List Available Notes',\n",
    "        button_style='info',\n",
    "        tooltip='List available notes for the given path/MRN'\n",
    "    )\n",
    "    \n",
    "    # Notes selection output\n",
    "    notes_output = widgets.Output()\n",
    "    \n",
    "    # Create the processing button\n",
    "    process_button = widgets.Button(\n",
    "        description='Generate Summaries',\n",
    "        button_style='primary',\n",
    "        tooltip='Click to process clinical notes'\n",
    "    )\n",
    "    \n",
    "    # Create output area\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Function to list available notes\n",
    "    def list_available_notes(b):\n",
    "        with notes_output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Get base path and MRN\n",
    "            base_path = base_path_input.value\n",
    "            mrn = mrn_input.value.strip()\n",
    "            \n",
    "            # Determine the actual path to use\n",
    "            if mrn:\n",
    "                actual_path = os.path.join(base_path, mrn)\n",
    "            else:\n",
    "                actual_path = base_path\n",
    "            \n",
    "            try:\n",
    "                # Check if directory exists\n",
    "                if not os.path.exists(actual_path):\n",
    "                    print(f\"Directory not found: {actual_path}\")\n",
    "                    return\n",
    "                \n",
    "                # Get all text files in the directory\n",
    "                files = list(Path(actual_path).glob(\"*.txt\"))\n",
    "                \n",
    "                if not files:\n",
    "                    print(f\"No text files found in {actual_path}\")\n",
    "                    return\n",
    "                \n",
    "                print(f\"Found {len(files)} text files in {actual_path}:\")\n",
    "                for i, file_path in enumerate(files, 1):\n",
    "                    print(f\"{i}. {file_path.name}\")\n",
    "                \n",
    "                # Show filter instructions\n",
    "                print(\"\\nUse the filter fields above to include or exclude notes based on keywords.\")\n",
    "                print(\"For example, enter 'Neurology, Psychiatry' in the include field to only process notes containing those terms.\")\n",
    "                print(\"Enter 'temp, draft' in the exclude field to skip notes containing those terms.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error listing notes: {e}\")\n",
    "    \n",
    "    # Function to filter notes based on include/exclude keywords\n",
    "    def filter_notes(notes_dict, include_keywords, exclude_keywords):\n",
    "        \"\"\"Filter notes based on include and exclude keywords\"\"\"\n",
    "        if not (include_keywords or exclude_keywords):\n",
    "            return notes_dict  # No filtering needed\n",
    "        \n",
    "        filtered_notes = {}\n",
    "        \n",
    "        for filename, content in notes_dict.items():\n",
    "            # Check exclude keywords first (if any keyword matches, skip this note)\n",
    "            if exclude_keywords:\n",
    "                if any(keyword.lower() in filename.lower() or \n",
    "                       keyword.lower() in content.lower() \n",
    "                       for keyword in exclude_keywords):\n",
    "                    continue\n",
    "            \n",
    "            # Then check include keywords (if any are specified, at least one must match)\n",
    "            if include_keywords:\n",
    "                if any(keyword.lower() in filename.lower() or \n",
    "                       keyword.lower() in content.lower() \n",
    "                       for keyword in include_keywords):\n",
    "                    filtered_notes[filename] = content\n",
    "            else:\n",
    "                # If no include keywords specified but passed exclude filter, include it\n",
    "                filtered_notes[filename] = content\n",
    "        \n",
    "        return filtered_notes\n",
    "    \n",
    "    # Define the processing function\n",
    "    def process_clinical_notes(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Get user inputs\n",
    "            base_path = base_path_input.value\n",
    "            mrn = mrn_input.value.strip()\n",
    "            model_id = model_dropdown.value\n",
    "            summary_mode = summary_type.value\n",
    "            token_limit = max_tokens.value\n",
    "            \n",
    "            # Process include/exclude keywords\n",
    "            include_keywords = [k.strip() for k in include_filter.value.split(',') if k.strip()]\n",
    "            exclude_keywords = [k.strip() for k in exclude_filter.value.split(',') if k.strip()]\n",
    "            \n",
    "            # Determine the actual path to use\n",
    "            if mrn:\n",
    "                directory_path = os.path.join(base_path, mrn)\n",
    "            else:\n",
    "                directory_path = base_path\n",
    "            \n",
    "            # Validate input\n",
    "            if not directory_path:\n",
    "                print(\"Please enter a valid directory path\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                # Get all clinical notes from the directory\n",
    "                print(f\"Loading clinical notes from {directory_path}...\")\n",
    "                all_clinical_notes = get_clinical_notes(directory_path)\n",
    "                \n",
    "                if not all_clinical_notes:\n",
    "                    print(f\"No clinical notes found in {directory_path}\")\n",
    "                    return\n",
    "                \n",
    "                # Apply filters if specified\n",
    "                if include_keywords or exclude_keywords:\n",
    "                    print(f\"Applying filters - Include: {include_keywords}, Exclude: {exclude_keywords}\")\n",
    "                    clinical_notes = filter_notes(all_clinical_notes, include_keywords, exclude_keywords)\n",
    "                    print(f\"Filtered from {len(all_clinical_notes)} to {len(clinical_notes)} notes\")\n",
    "                else:\n",
    "                    clinical_notes = all_clinical_notes\n",
    "                \n",
    "                if not clinical_notes:\n",
    "                    print(\"No notes remain after filtering. Please adjust your filter criteria.\")\n",
    "                    return\n",
    "                \n",
    "                if summary_mode == 'Individual summaries':\n",
    "                    # Create a DataFrame to store results\n",
    "                    results = []\n",
    "                    \n",
    "                    # Process each clinical note individually\n",
    "                    print(f\"Generating individual summaries using {model_id}...\")\n",
    "                    for filename, note_text in tqdm(clinical_notes.items(), desc=\"Processing\"):\n",
    "                        # Check if text is not empty\n",
    "                        if note_text.strip():\n",
    "                            # Generate summary\n",
    "                            summary = summarize_with_bedrock(note_text, model_id=model_id, max_tokens=1000)\n",
    "                            \n",
    "                            # Save the summary to a text file with the same name + _individual_summary\n",
    "                            summary_filename = os.path.splitext(filename)[0] + \"_individual_summary.txt\"\n",
    "                            summary_filepath = os.path.join(directory_path, summary_filename)\n",
    "                            \n",
    "                            with open(summary_filepath, 'w', encoding='utf-8') as f:\n",
    "                                f.write(f\"SUMMARY OF {filename}\\n\")\n",
    "                                f.write(f\"Generated using {model_id}\\n\\n\")\n",
    "                                f.write(summary)\n",
    "                            \n",
    "                            # Append result to DataFrame\n",
    "                            results.append({\n",
    "                                \"filename\": filename,\n",
    "                                \"summary_filename\": summary_filename,\n",
    "                                \"original_text\": note_text[:100] + \"...\" if len(note_text) > 100 else note_text,\n",
    "                                \"summary\": summary\n",
    "                            })\n",
    "                        else:\n",
    "                            print(f\"Skipping empty file: {filename}\")\n",
    "                    \n",
    "                    # Create DataFrame\n",
    "                    results_df = pd.DataFrame(results)\n",
    "                    \n",
    "                    # Save the results to a CSV file in the same directory\n",
    "                    output_path = os.path.join(directory_path, \"clinical_summaries_individual.csv\")\n",
    "                    results_df.to_csv(output_path, index=False)\n",
    "                    print(f\"Individual summaries saved to {directory_path}\")\n",
    "                    print(f\"Summary index saved to {output_path}\")\n",
    "                    \n",
    "                    # Display the DataFrame\n",
    "                    display(results_df)\n",
    "                    \n",
    "                else:  # Combined summary\n",
    "                    # Generate summary using chunking approach\n",
    "                    print(f\"\\nGenerating combined summary using {model_id}...\")\n",
    "                    comprehensive_summary = summarize_combined_notes_chunked(\n",
    "                        clinical_notes, \n",
    "                        model_id=model_id, \n",
    "                        max_tokens=token_limit,\n",
    "                        max_input_tokens=7500  # Conservative limit to avoid errors\n",
    "                    )\n",
    "                    \n",
    "                    # Save the combined summary to a text file\n",
    "                    output_path = os.path.join(directory_path, \"clinical_summary_combined.txt\")\n",
    "                    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(f\"COMBINED SUMMARY OF {len(clinical_notes)} CLINICAL NOTES\\n\")\n",
    "                        f.write(f\"Generated using {model_id}\\n\\n\")\n",
    "                        f.write(comprehensive_summary)\n",
    "                        \n",
    "                    print(f\"\\nCombined summary saved to {output_path}\")\n",
    "                    \n",
    "                    # Display the summary\n",
    "                    print(\"\\n==== COMPREHENSIVE SUMMARY ====\")\n",
    "                    print(comprehensive_summary)\n",
    "                    \n",
    "                # Prompt the user to proceed to the chat interface\n",
    "                print(\"\\nSummaries have been generated. You can now proceed to the chat interface to ask questions about the notes.\")\n",
    "                initialize_chat_ui(directory_path, model_id)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing clinical notes: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Connect the button click events\n",
    "    list_notes_button.on_click(list_available_notes)\n",
    "    process_button.on_click(process_clinical_notes)\n",
    "    \n",
    "    # Display the widgets\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Step 4: Generate Summaries</h3>\"),\n",
    "        widgets.HTML(\"<p>Configure the summarization process below:</p>\"),\n",
    "        base_path_input,\n",
    "        mrn_input,\n",
    "        list_notes_button,\n",
    "        notes_output,\n",
    "        widgets.HTML(\"<h4>Filter Options</h4>\"),\n",
    "        include_filter,\n",
    "        exclude_filter,\n",
    "        widgets.HTML(\"<h4>Summarization Options</h4>\"),\n",
    "        model_dropdown,\n",
    "        summary_type,\n",
    "        max_tokens,\n",
    "        process_button,\n",
    "        output\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81df68d-4da3-44b9-abaf-b32a104eab01",
   "metadata": {},
   "source": [
    "## Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe0d67e1-c744-47f4-a410-2553c1f07370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4328377c35342fea55e54262fc34a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h2>Clinical Notes Pipeline</h2>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d25ef85ecc647ea8c9e93657915d255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p>Click on each step button to execute that part of the pipeline:</p>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ac15932c504790911342a1203fc493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Step 1: Load Data', layout=Layout(margin='10px 0px'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c8bcbc7cbe4683a940bc8a784f8738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Global variables for chat context and AWS clients\n",
    "chat_context = None\n",
    "bedrock_runtime = None\n",
    "bedrock = None\n",
    "\n",
    "# Main Pipeline Execution\n",
    "def run_pipeline():\n",
    "    \"\"\"Initialize the complete clinical notes pipeline with step buttons\"\"\"\n",
    "    # Create step buttons\n",
    "    step1_button = widgets.Button(\n",
    "        description='Step 1: Load Data',\n",
    "        button_style='primary',\n",
    "        tooltip='Extract notes from CSV',\n",
    "        layout={'width': 'auto', 'margin': '10px 0px'}\n",
    "    )\n",
    "    \n",
    "    step2_button = widgets.Button(\n",
    "        description='Step 2: Filter Notes',\n",
    "        button_style='primary',\n",
    "        tooltip='Filter extracted notes',\n",
    "        layout={'width': 'auto', 'margin': '10px 0px'}\n",
    "    )\n",
    "    \n",
    "    step3_button = widgets.Button(\n",
    "        description='Step 3: Configure AWS',\n",
    "        button_style='primary',\n",
    "        tooltip='Set up AWS Bedrock',\n",
    "        layout={'width': 'auto', 'margin': '10px 0px'}\n",
    "    )\n",
    "    \n",
    "    step4_button = widgets.Button(\n",
    "        description='Step 4: Generate Summaries',\n",
    "        button_style='primary',\n",
    "        tooltip='Summarize clinical notes',\n",
    "        layout={'width': 'auto', 'margin': '10px 0px'}\n",
    "    )\n",
    "    \n",
    "    step5_button = widgets.Button(\n",
    "        description='Step 5: Chat with Notes',\n",
    "        button_style='primary',\n",
    "        tooltip='Ask questions about the notes',\n",
    "        layout={'width': 'auto', 'margin': '10px 0px'}\n",
    "    )\n",
    "    \n",
    "    # Step output area\n",
    "    step_output = widgets.Output()\n",
    "    \n",
    "    # Connect button click handlers\n",
    "    step1_button.on_click(lambda b: run_step1(step_output))\n",
    "    step2_button.on_click(lambda b: run_step2(step_output))\n",
    "    step3_button.on_click(lambda b: run_step3(step_output))\n",
    "    step4_button.on_click(lambda b: run_step4(step_output))\n",
    "    step5_button.on_click(lambda b: run_step5(step_output))\n",
    "    \n",
    "    # Display the pipeline interface\n",
    "    display(widgets.HTML(\"<h2>Clinical Notes Pipeline</h2>\"))\n",
    "    display(widgets.HTML(\"<p>Click on each step button to execute that part of the pipeline:</p>\"))\n",
    "    display(widgets.VBox([\n",
    "        step1_button,\n",
    "        step2_button,\n",
    "        step3_button,\n",
    "        step4_button,\n",
    "        step5_button\n",
    "    ]))\n",
    "    display(step_output)\n",
    "\n",
    "# Step execution functions\n",
    "def run_step1(output):\n",
    "    \"\"\"Run Step 1: Load Data\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"Executing Step 1: Load Data...\")\n",
    "        initialize_extraction()\n",
    "\n",
    "def run_step2(output):\n",
    "    \"\"\"Run Step 2: Filter Notes\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"Executing Step 2: Filter Notes...\")\n",
    "        initialize_filtering_widgets()\n",
    "\n",
    "def run_step3(output):\n",
    "    \"\"\"Run Step 3: Configure AWS\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"Executing Step 3: Configure AWS...\")\n",
    "        setup_aws_ui()\n",
    "\n",
    "def run_step4(output):\n",
    "    \"\"\"Run Step 4: Generate Summaries\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"Executing Step 4: Generate Summaries...\")\n",
    "        initialize_summarization_ui()\n",
    "\n",
    "def run_step5(output):\n",
    "    \"\"\"Run Step 5: Chat with Notes\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"Executing Step 5: Chat with Notes...\")\n",
    "        # Use the last directory path from summarization, or default to notes_dir\n",
    "        directory_path = notes_dir\n",
    "        # Use a default model if none was selected\n",
    "        model_id = \"anthropic.claude-v2\"\n",
    "        initialize_chat_ui(directory_path, model_id)\n",
    "\n",
    "# Example questions for the chat interface\n",
    "def show_example_questions():\n",
    "    \"\"\"Display example questions that can be asked about the clinical notes\"\"\"\n",
    "    example_questions = [\n",
    "        \"What are the primary diagnoses mentioned in the notes?\",\n",
    "        \"What medications are prescribed to the patients?\",\n",
    "        \"Are there any allergies mentioned in the notes?\",\n",
    "        \"What treatments have been recommended?\",\n",
    "        \"What symptoms are most frequently mentioned?\",\n",
    "        \"Were any surgical procedures performed?\",\n",
    "        \"What is the patient's medical history?\",\n",
    "        \"What follow-up recommendations were given?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Example questions you can ask about the clinical notes:\")\n",
    "    for i, question in enumerate(example_questions, 1):\n",
    "        print(f\"{i}. {question}\")\n",
    "\n",
    "# Run the pipeline when the notebook is executed\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862c05a-b487-405a-ba8d-5efd759beb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ec783-bf2a-41c9-98a5-31a1e74837fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
